# üöÄ Deployment Guide

## Vercel Deployment

### ‚ö†Ô∏è Important Note about Direct Llama on Vercel
**Direct Llama cannot run on Vercel** due to serverless function limitations:
- 50MB deployment size limit (model is ~50MB alone)
- 10-second execution timeout (model loading takes longer)
- No persistent file system for model caching

For Vercel deployment, use **OpenAI** or **Groq** providers instead.

### üîß Deploy to Vercel

1. **Install Vercel CLI:**
   ```bash
   npm install -g vercel
   ```

2. **Login to Vercel:**
   ```bash
   vercel login
   ```

3. **Deploy:**
   ```bash
   npm run deploy
   ```

4. **Set Environment Variables in Vercel Dashboard:**
   - `AI_PROVIDER=openai`
   - `OPENAI_API_KEY=your_key_here`
   - `DB_TYPE=memory`

### üåê Alternative: Deploy with OpenAI

1. **Update your local .env for testing:**
   ```bash
   AI_PROVIDER=openai
   OPENAI_API_KEY=your_openai_key_here
   ```

2. **Test locally:**
   ```bash
   npm run build:all
   npm start
   ```

3. **Deploy to Vercel:**
   ```bash
   vercel --prod
   ```

### üê≥ Docker Deployment (Recommended for Direct Llama)

For Direct Llama integration, use Docker deployment instead:

```bash
# Build and run with Docker
docker build -t ai-notes .
docker run -p 8080:8080 -e AI_PROVIDER=llama ai-notes
```

### üîÑ Hybrid Approach

**Local Development:** Use Direct Llama for privacy and cost
**Production:** Use OpenAI/Groq for reliability and performance

```bash
# Local
AI_PROVIDER=llama

# Production
AI_PROVIDER=openai
OPENAI_API_KEY=your_key_here
```

## Other Deployment Options

### Railway
- Supports Direct Llama
- Persistent storage
- Easy deployment from GitHub

### Render
- Free tier available
- Supports larger deployments
- Good for Direct Llama

### DigitalOcean App Platform
- Supports Docker
- Scalable
- Good for production Direct Llama deployment